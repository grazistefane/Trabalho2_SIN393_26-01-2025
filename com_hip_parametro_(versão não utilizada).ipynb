{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mbWlZBviROp"
      },
      "outputs": [],
      "source": [
        "#importando as bbts\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "import random\n",
        "import itertools\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iNHlg7ZvNdr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf_k0pKTmd5v"
      },
      "outputs": [],
      "source": [
        "#caminho para o dataset\n",
        "dataset_path = '/content/drive/MyDrive/trabalho_visao/BrainTumor_MRI_Scans'\n",
        "\n",
        "#configurações principais\n",
        "batch_size = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#transformações de imagem\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  #redimensiona as imagens para 128 por 128\n",
        "    transforms.RandomHorizontalFlip(),  #aplica flip horizontal aleatório\n",
        "    transforms.RandomRotation(10),  #aplica rotação aleatória de até 10 graus\n",
        "    transforms.ToTensor(),  #converte para tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  #normaliza com os valores de ImageNet\n",
        "])\n",
        "\n",
        "\n",
        "#carregando o dataset\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_class_samples(dataset, num_samples=3):\n",
        "    #obter as classes do dataset\n",
        "    classes = dataset.classes\n",
        "\n",
        "    #configuração do gráfico\n",
        "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(num_samples * 3, len(classes) * 3))\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        #filtra as imagens da classe\n",
        "        class_idx = dataset.class_to_idx[cls]\n",
        "        #obtém caminhos das imagens e verifica se o arquivo existe\n",
        "        class_images = [img for img, label in dataset.samples if label == class_idx and os.path.isfile(img)]\n",
        "\n",
        "        # Verifica se há imagens suficientes para a classe\n",
        "        if len(class_images) < num_samples:\n",
        "            print(f\"Erro: A classe '{cls}' tem apenas {len(class_images)} imagens disponíveis. Ajustando para exibir todas.\")\n",
        "            selected_images = class_images  #exibe todas as disponíveis\n",
        "        else:\n",
        "            selected_images = random.sample(class_images, num_samples)  #seleciona imagens aleatórias\n",
        "\n",
        "        for j, img_path in enumerate(selected_images):\n",
        "            #abre e exibe a imagem\n",
        "            img = Image.open(img_path)  #img_path é o caminho da imagem\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].axis('off')\n",
        "            if j == 0:\n",
        "                axes[i, j].set_title(f\"{cls} imagens\", fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#exibindo amostras para as classes do dataset\n",
        "show_class_samples(dataset)"
      ],
      "metadata": {
        "id": "7E4AFNzlyRwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dividindo o dataset em treino, validação e teste\n",
        "train_val_dataset, test_dataset = train_test_split(dataset, test_size=0.15, random_state=42)\n",
        "train_dataset, val_dataset = train_test_split(train_val_dataset, test_size=0.15, random_state=42)\n",
        "\n",
        "#criando DataLoaders para cada conjunto\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "V-jlF3XEfiNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#função para treinar o modelo\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            #zera os gradientes\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #forward e cálculo de perda\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            #backward e atualização dos pesos\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #estatísticas\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = 100.0 * correct / total\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    return model"
      ],
      "metadata": {
        "id": "pEDoPQKlJwhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0IdrcmDHbcb"
      },
      "outputs": [],
      "source": [
        "#função para avaliar o modelo\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_labels), np.array(all_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "892bFNiWgT2I"
      },
      "outputs": [],
      "source": [
        "def cross_validate_model(model, dataset, k_folds=5, num_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f\"Fold {fold + 1}/{k_folds}\")\n",
        "\n",
        "        #cria DataLoaders para o conjunto de treino e validação\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "        val_loader = DataLoader(dataset, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "        #cria uma nova cópia do modelo para cada fold\n",
        "        model_fold = deepcopy(model).to(device)\n",
        "\n",
        "        #configura critério e otimizador\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model_fold.parameters(), lr=0.001)\n",
        "\n",
        "        #treina o modelo\n",
        "        model_fold = train_model(model_fold, train_loader, criterion, optimizer, num_epochs)\n",
        "\n",
        "        #avalia no conjunto de validação\n",
        "        y_true, y_pred = evaluate_model(model_fold, val_loader, device)\n",
        "\n",
        "        #matriz de confusão\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
        "        plt.title(f'Matriz de Confusão para Fold {fold + 1}')\n",
        "        plt.xlabel('Predito')\n",
        "        plt.ylabel('Real')\n",
        "        plt.show()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        #relatório de classificação\n",
        "        print(f\"\\nRelatório de Classificação para Fold {fold + 1}:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=dataset.classes))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_hyperparameters(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)  # Multiplicar pelo tamanho do batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)  # Dividir pelo total de amostras\n",
        "        epoch_acc = 100.0 * correct / total\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "16Nr1Ylp-vRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(model, dataset, param_grid, k_folds=5, num_epochs=10):\n",
        "    best_model = None\n",
        "    best_acc = 0.0\n",
        "    best_params = {}\n",
        "\n",
        "    #gerar todas as combinações de hiperparâmetros\n",
        "    param_combinations = list(itertools.product(*param_grid.values()))\n",
        "\n",
        "    #loop por todas as combinações\n",
        "    for params in param_combinations:\n",
        "        print(f\"Testando hiperparâmetros: {dict(zip(param_grid.keys(), params))}\")\n",
        "\n",
        "        #ajusta o modelo e os hiperparâmetros\n",
        "        learning_rate = params[0]\n",
        "        num_epochs = params[1]\n",
        "        batch_size = params[2]\n",
        "\n",
        "        #divide o dataset em treino e validação\n",
        "        train_val_dataset, test_dataset = train_test_split(dataset, test_size=0.15, random_state=42)\n",
        "        train_dataset, val_dataset = train_test_split(train_val_dataset, test_size=0.15, random_state=42)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        #recria o modelo e otimizador para cada combinação\n",
        "        model_copy = deepcopy(model).to(device)  # Garante que o modelo seja transferido para o dispositivo\n",
        "        optimizer = torch.optim.Adam(model_copy.parameters(), lr=learning_rate)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        #treina o modelo com os hiperparâmetros atuais\n",
        "        model_copy = train_model_with_hyperparameters(model_copy, train_loader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "        #avalia o modelo no conjunto de validação\n",
        "        y_true, y_pred = evaluate_model(model_copy, val_loader, device)\n",
        "        accuracy = np.mean(y_true == y_pred)\n",
        "\n",
        "        print(f\"Precisão no conjunto de validação: {accuracy:.2f}\")\n",
        "\n",
        "        #se o modelo atual for o melhor, salvar\n",
        "        if accuracy > best_acc:\n",
        "            best_acc = accuracy\n",
        "            best_model = model_copy\n",
        "            best_params = dict(zip(param_grid.keys(), params))\n",
        "\n",
        "    print(f\"Melhores Hiperparâmetros: {best_params}\")\n",
        "    return best_model, best_params\n"
      ],
      "metadata": {
        "id": "zg1M7gje-7Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSeR6RHnhPu_"
      },
      "outputs": [],
      "source": [
        "#treinando e avaliando os modelos com validação cruzada\n",
        "models_to_train = {\n",
        "    \"VGG16\": models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1),\n",
        "    \"ResNet18\": models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1),\n",
        "}\n",
        "\n",
        "#ajusta a última camada do VGG16\n",
        "num_classes = len(dataset.classes)\n",
        "models_to_train[\"VGG16\"].classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "#ajusta a última camada do ResNet18\n",
        "models_to_train[\"ResNet18\"].fc = nn.Linear(models_to_train[\"ResNet18\"].fc.in_features, num_classes)\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.0001],  #taxa de aprendizado\n",
        "    'num_epochs': [5, 10],  #número de épocas\n",
        "    'batch_size': [8, 16]  #tamanho do batch\n",
        "}\n",
        "\n",
        "#chama o Grid Search para encontrar os melhores hiperparâmetros\n",
        "for model_name, model in models_to_train.items():\n",
        "    print(f\"\\nTreinando Modelo: {model_name}\")\n",
        "\n",
        "    #realiza o grid search e obtém o melhor modelo e os melhores parâmetros\n",
        "    best_model, best_params = grid_search(model, dataset, param_grid)\n",
        "\n",
        "    #libera memória após grid search\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    #exibe os melhores hiperparâmetros encontrados\n",
        "    print(f\"Melhores Hiperparâmetros para o modelo {model_name}: {best_params}\")\n",
        "\n",
        "    #treinamento e validação cruzada com os melhores parâmetros\n",
        "    print(f\"\\nTreinando com validação cruzada - Modelo: {model_name}\")\n",
        "    cross_validate_model(best_model, dataset, k_folds=5, num_epochs=best_params['num_epochs'])\n",
        "\n",
        "    #libera memória antes de passar para o próximo modelo\n",
        "    del best_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
